%% MMSP2 - Lab 1
%  Exercise 1 - Audio signal encoding

clearvars
close all
clc

%% 1) Load file 'pf.wav' and plot it. 
%%    Check that its values are included between [-1,+1]
[x,Fs] = audioread('pf.wav');

t = linspace(1, length(x)/Fs, length(x));

figure()
plot(t, x)
ylabel('Amplitude')
xlabel('Time [s]')

disp(['Fs: ' num2str(Fs)]);
disp(['size(x): ' mat2str(size(x))]);
fprintf('min(x): %.1f\nmax(x): %.1f\n',min(x),max(x));

%% 2) Take the first 60 s of the file and rescale its values 
%% between [0 255], rounding to the nearest integer

dur = 60;
x60 = x(1:Fs*dur);
disp(['size(x60): ' mat2str(size(x60))]);

min_in = -1;
max_in = 1;
min_out = 0;
max_out = 255;

x60_255 = round((x60-min_in)/(max_in-min_in)*(max_out-min_out)+min_out);

fprintf('min(x60_255): %.1f\nmax(x60_255): %.1f\n',min(x60_255),max(x60_255));

%% 3) Convert each value into its binary representation over 8 bit
%%    hint: use the function de2bi() or dec2bin() - '0' if you don't have the Communications System toolbox

x60_255_bi = de2bi(x60_255);

% x60_255_bi = dec2bin(x60_255) - '0';

disp(['size(x60_255_bi): ' mat2str(size(x60_255_bi))]);

%% 4) Display the entropy of the binary source that has generated the signal
%%    hint: use the function hist() to compute the probability distribution

alphabet_bi = 0:1;

d_bi = hist(x60_255_bi(:), alphabet_bi);

disp(['size(d_bi): ' mat2str(size(d_bi))]);

p_bi = d_bi/sum(d_bi);

figure()
bar(alphabet_bi, p_bi);
xlabel('Symbol');
ylabel('Probability');
grid('on');

H_bi = -sum(p_bi .* log2(p_bi));

fprintf('entropy: %.3f bit/symbol\n',H_bi);

%% 5) Consider the signal as generated by a finite source with alphabet [0 255].
%%    Plot the normalized histogram and compute the entropy.
%%    hint: avoid evaluating log2(0) when computing the entropy!

% Same procedure but with different alphabet

alphabet_255 = 0:255;
d_255 = hist(x60_255(:),alphabet_255);
disp(['size(d_255): ' mat2str(size(d_255))]);

p_255 = d_255/sum(d_255);

figure()
bar(alphabet_255, p_255);
xlabel('Symbol');
ylabel('Probability');
grid('on');

% We are not guaranteed that we meed all the symbols of our alphabet
% (Problem since we have to compute a log)
p_255 = p_255(d_255 > 0);

H_255 = -sum(p_255 .* log2(p_255));

fprintf('entropy: %.3f bit/symbol <= 8 bit/symbol\n',H_255);

%% 6) Consider the signal as generated by a source with memory=1.
%%    Compute the conditional entropy

j = x60_255;
k = [0; x60_255(1:end-1)];

% Compute joint probability

d_joint = hist3([j, k], {alphabet_255, alphabet_255});
disp(['size(d_joint): ' mat2str(size(d_joint))]);

p_joint = d_joint / sum(d_joint(:));

figure();
imagesc(db(p_joint)), axis xy;
title('Joint PMF');
ylabel('j symbols');
xlabel('k symbols');

figure()
surf(db(p_joint)), axis xy;
title('Joint PMF');
ylabel('j symbols');
xlabel('k symbols');

% compute joint entropy
H_joint = -sum(sum(p_joint(d_joint > 0) .* log2(p_joint(d_joint > 0))));
fprintf('joint entropy: %.3f bit/2 symbols <= 16 bit/2 symbols\n',H_joint);

% compute conditional entropy (K|J) using chain rule

d_j = hist(j(:), alphabet_255);
p_j = d_j / sum(d_j);
H_j = -sum(p_j(d_j>0) .* log2(p_j(d_j > 0)));

H_cond_cr = H_joint - H_j;
fprintf('cond entropy by chain rule: %.3f bit/symbol\n',H_cond_cr);

%% compute conditional entropy by definition

d_j_ext = repmat(d_j, 256, 1);
p_j_ext = repmat(p_j, 256, 1);
non_zero_mask = d_joint > 0 & d_j_ext > 0;

H_cond = -sum(sum(p_joint(non_zero_mask) .* log2(p_joint(non_zero_mask)./p_j_ext(non_zero_mask))));
fprintf('cond entropy by definition: %.3f bit/symbol\n',H_cond);




